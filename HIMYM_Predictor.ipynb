{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HIMYM Predictor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOy95RLYo22xDx5hRYI987Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonasmue/nlp-playground/blob/master/HIMYM_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YyKpfXsZ80j",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Initial Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD-N15EyZm9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j3V3jb-dT1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1yImB5Z15J",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhvPDsOFaCNz",
        "colab_type": "text"
      },
      "source": [
        "### Load Data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFnf9LDYacuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "31ddfcdb-a463-4d71-9210-4405fcbac8b4"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy5pYfKGaz2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a615d0a-2276-4e05-8455-47f0e9d6773c"
      },
      "source": [
        "!ls drive/My\\ Drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   data  'Getting started.pdf'   results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIgclcEpag24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = os.path.join(\"drive\", \"My Drive\", \"data\", \"himym.txt\")\n",
        "with open(data_path, \"r\") as input_file:\n",
        "  text = input_file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eId6V1_bH0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f786b846-66cb-4017-92e4-a49b35398b02"
      },
      "source": [
        "# Print the first letters of the text\n",
        "text[:150]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n01x01 - Pilot\\n\\n\\nPilot\\nScene One\\n[Title: The Year 2030]\\nNarrator: Kids, I'm going to tell you an incredible story. The story of how I met your mothe\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phKlEJgFaGW0",
        "colab_type": "text"
      },
      "source": [
        "### Turn Data into Label Encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzCs9k1RbY3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convenience Dictionaries\n",
        "characters = set(text)\n",
        "id2char = dict(enumerate(characters))\n",
        "char2id = {c:i for i,c in enumerate(characters)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVSUu3DJbfCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert char2id[id2char[5]] == 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcWbdjCxcM0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_characters = len(characters)\n",
        "text_labels = [char2id[c] for c in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSEeBaUfcXRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "790c8234-64a4-473c-b4b8-0a3036a451a0"
      },
      "source": [
        "print(\"The text consists of {} distinct characters.\".format(num_characters))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text consists of 93 distinct characters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi6t2XiKaKFC",
        "colab_type": "text"
      },
      "source": [
        "### Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UIuqJVKcn8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(text_labels, num_characters):\n",
        "  eye = torch.eye(num_characters)\n",
        "  X = torch.zeros((text_labels.shape[0], text_labels.shape[1], num_characters))\n",
        "  for i, sentence_labels in enumerate(text_labels):\n",
        "    X[i] = eye[sentence_labels]\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFcuFKCQcowk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outputs tensor of X with shape [batch_size, seq_len, num_chars] and y with shape [batch_size, seq_len]\n",
        "def get_next_training_batch(labels=text_labels, num_chars=num_characters, seq_len=128, batch_size=32):\n",
        "  \"\"\"\n",
        "  Returns a training batch generator which itself returns batches with\n",
        "  tuples of the following format\n",
        "\n",
        "  X of shape [batch_size, seq_len, num_chars] (one-hot-encoded) and\n",
        "  y of shape [batch_size, seq_len] (label-encoded)\n",
        "\n",
        "  Arguments:\n",
        "    labels: label encodings of the text to create batches from\n",
        "    num_chars: the total number of characters\n",
        "    seq_len: the length of the character sequence of each batch\n",
        "    batch_size: the number of character sequences per batch\n",
        "  \"\"\"\n",
        "  for batch_offset in range(0, len(labels), batch_size * (seq_len + 1)):\n",
        "    if len(labels) < batch_offset + batch_size * (seq_len + 1):\n",
        "      return\n",
        "    batch = labels[batch_offset:batch_offset + batch_size * (seq_len + 1)]\n",
        "    X_text_labels = torch.Tensor([batch[i:i+seq_len] for i in range(0, len(batch), seq_len + 1)]).long()\n",
        "    X_one_hot = to_one_hot(X_text_labels, num_characters)\n",
        "    y_text_labels = torch.Tensor([batch[i+1:i+seq_len+1] for i in range(0, len(batch), seq_len + 1)]).long()\n",
        "    yield X_one_hot.to(dev), y_text_labels.to(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8859QXpTc-Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the implementation to see if it generates valid outpus\n",
        "X_sample, y_sample = next(get_next_training_batch(seq_len=8, batch_size=5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY8zp8rjdmpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert X_sample.shape == torch.Size([5, 8, num_characters])\n",
        "assert y_sample.shape == torch.Size([5, 8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfRWwpBXd1Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert X_sample[0, 1].argmax().item() == y_sample[0][0]\n",
        "assert X_sample[1, 2].argmax().item() == y_sample[1][1]\n",
        "assert X_sample[4, 7].argmax().item() == y_sample[4][6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or-yGW4se1gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_to_text(tensor):\n",
        "  \"\"\"\n",
        "  Converts a tensor representation back to a string representation.\n",
        "\n",
        "  Arguments:\n",
        "    tensor: a torch.Tensor object with the following shape:\n",
        "      3D: [batch_size, seq_len, num_chars]\n",
        "      2D: [batch_size, seq_len]\n",
        "      1D: [seq_len]\n",
        "  \"\"\"\n",
        "  if len(tensor.shape) == 3:\n",
        "    return tensor_to_text(tensor.argmax(dim=2))\n",
        "  if len(tensor.shape) == 2:\n",
        "    return [tensor_to_text(line) for line in tensor]\n",
        "  if len(tensor.shape) == 1:\n",
        "    return \"\".join([tensor_to_text(char_encoding) for char_encoding in tensor])\n",
        "  if len(tensor.shape) == 0:\n",
        "    return id2char[tensor.item()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exyHWyLQgv9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e6eee086-a64d-47ca-a930-37bb38ef8234"
      },
      "source": [
        "print(\"3D:\", tensor_to_text(X_sample))\n",
        "print(\"2D:\", tensor_to_text(y_sample))\n",
        "print(\"1D:\", tensor_to_text(torch.Tensor([char2id[\"J\"], char2id[\"o\"], char2id[\"n\"], char2id[\"a\"], char2id[\"s\"]])))\n",
        "print(\"0D:\", tensor_to_text(torch.tensor(char2id[\"J\"])))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3D: ['\\n\\n\\n01x01', '- Pilot\\n', '\\nPilot\\nS', 'ene One\\n', 'Title: T']\n",
            "2D: ['\\n\\n01x01 ', ' Pilot\\n\\n', 'Pilot\\nSc', 'ne One\\n[', 'itle: Th']\n",
            "1D: Jonas\n",
            "0D: J\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0PT7MnzAGhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67f22798-b9ed-4fbb-d8af-8fd15b60edca"
      },
      "source": [
        "torch.Tensor([15])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICUOVqmaaQgD",
        "colab_type": "text"
      },
      "source": [
        "# Custom GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs5Eq1h8kgam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUCell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Weights and Biases\n",
        "    # See https://en.wikipedia.org/wiki/Gated_recurrent_unit#Fully_gated_unit\n",
        "    \n",
        "    ## z\n",
        "    self.W_xz = nn.Parameter(torch.zeros(self.input_size, self.hidden_size))\n",
        "    self.U_hz = nn.Parameter(torch.zeros(self.hidden_size, self.hidden_size))\n",
        "    self.b_z = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "    ## r\n",
        "    self.W_xr = nn.Parameter(torch.zeros_like(self.W_xz))\n",
        "    self.U_hr = nn.Parameter(torch.zeros_like(self.U_hz))\n",
        "    self.b_r = nn.Parameter(torch.zeros_like(self.b_z))\n",
        "\n",
        "    ## h\n",
        "    self.W_xh = nn.Parameter(torch.zeros_like(self.W_xz))\n",
        "    self.U_hh = nn.Parameter(torch.zeros_like(self.U_hz))\n",
        "    self.b_h = nn.Parameter(torch.zeros_like(self.b_z))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for weight in self.parameters():\n",
        "      if len(weight.shape) > 1:\n",
        "        # Init matrices with random noise\n",
        "        nn.init.xavier_normal_(weight)\n",
        "      else:\n",
        "        # Init biases with zeros\n",
        "        nn.init.zeros_(weight)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    h = nn.Parameter(torch.zeros((batch_size, self.hidden_size))).to(dev)\n",
        "    nn.init.zeros_(h)\n",
        "    return h\n",
        "\n",
        "  def forward(self, x, h=None):\n",
        "    \"\"\"\n",
        "    Argument shapes:\n",
        "    x is of shape [batch_size, input_size]\n",
        "    h is of shape [batch_size, hidden_size]\n",
        "\n",
        "    Output shape:\n",
        "    h is of shape [batch_size, hidden_size]\n",
        "    \"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "\n",
        "    if h is None:\n",
        "      h = self.init_hidden(x.shape[0])\n",
        "    \n",
        "    z = torch.sigmoid(x.mm(self.W_xz) + h.mm(self.U_hz) + self.b_z)\n",
        "    r = torch.sigmoid(x.mm(self.W_xr) + h.mm(self.U_hr) + self.b_r)\n",
        "    h = z * h + (1 - z) * torch.tanh(x.mm(self.W_xh) + (r * h).mm(self.U_hh) + self.b_h)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HspvC9QdkB0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, num_characters=num_characters, hidden_size=512, batch_first=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_characters = num_characters\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_first = batch_first\n",
        "\n",
        "    self.cell = GRUCell(num_characters, hidden_size)\n",
        "    self.dense = nn.Linear(self.hidden_size, self.num_characters)\n",
        "\n",
        "  def forward(self, X, h_0=None):\n",
        "    \"\"\"\n",
        "    Argument shapes:\n",
        "    X is of shape [batch_size, seq_len, num_chars] if self.batch_first\n",
        "    X is of shape [seq_len, batch_size, num_chars] if not self.batch_first \n",
        "    ---\n",
        "    h is of shape [batch_size, hidden_size]\n",
        "\n",
        "    Output shapes:\n",
        "    y_hat is of shape [batch_size * seq_len, num_chars]\n",
        "    h_t is of shape [batch_size, hidden_size]\n",
        "    \"\"\"\n",
        "    assert len(X.shape) == 3\n",
        "\n",
        "    # Put seq_len in the front\n",
        "    if self.batch_first:\n",
        "      X = X.permute(1, 0, 2)\n",
        "      # X is now of shape [seq_len, batch_size, num_chars]\n",
        "\n",
        "    h_t = h_0\n",
        "    output = torch.zeros((X.shape[0], X.shape[1], self.hidden_size)).to(dev)\n",
        "    for t, x_t in enumerate(X):\n",
        "      # Iterate over sequence\n",
        "      h_t = self.cell(x_t, h_t)\n",
        "      output[t] = h_t # [batch_size, hidden_size]\n",
        "    \n",
        "    # TODO: Permute output back?!\n",
        "    output = output.permute(1, 0, 2)\n",
        "\n",
        "    output = output.contiguous().view(-1, self.hidden_size) # [batch_size * seq_len, hidden_size]\n",
        "    y_hat = self.dense(output) # [batch_size * seq_len, num_chars]\n",
        "    return y_hat, h_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HdjU5HUaUD2",
        "colab_type": "text"
      },
      "source": [
        "# Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMFdO08lzO4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(rnn, char, h):\n",
        "  x = to_one_hot(torch.LongTensor([[char2id[char]]]), num_characters).to(dev)\n",
        "  y_hat, h = rnn(x, h)\n",
        "  next_char = tensor_to_text(torch.softmax(y_hat, dim=-1).argmax())[0]\n",
        "  return next_char, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjB4Yn44xpYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_text(rnn, h=None, seq_len=150, starting_character=\"\\n\"):\n",
        "  current_char = starting_character\n",
        "  result = [current_char]\n",
        "  for i in range(seq_len):\n",
        "    current_char, h = predict_next_char(rnn, current_char, h)\n",
        "    result.append(current_char)\n",
        "  return \"\".join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK_ZjPMqaWvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rnn, n_epochs=50, learning_rate=2e-3, print_every=100, batch_size=64, seq_len=128, predict_len=150):\n",
        "  outname = os.path.join(\"drive\", \"My Drive\", \"results\", str(datetime.now()) + \".txt\")\n",
        "  with open(outname, \"w\") as f:\n",
        "    f.write(\"Training {}, num layers: {}, hidden size: {}, batch size: {}, sequence length: {}\".format(str(datetime.now()), 1, rnn.hidden_size, batch_size, seq_len))\n",
        "\n",
        "  rnn.train()\n",
        "\n",
        "  step = 0\n",
        "  losses = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    h = None\n",
        "    for X, y in get_next_training_batch(seq_len=seq_len, batch_size=batch_size):\n",
        "      step += 1\n",
        "      rnn.zero_grad()\n",
        "      \n",
        "      y_hat, h = rnn(X, h)\n",
        "      h = h.data # in order to not go through entire history\n",
        "\n",
        "      loss = criterion(y_hat, y.view(batch_size * seq_len))\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradient clipping\n",
        "      nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
        "      optimizer.step()\n",
        "\n",
        "      if not step % print_every:\n",
        "        rnn.eval()\n",
        "        running_loss = sum(losses) / len(losses)\n",
        "        losses = []\n",
        "        out_string = \"\\n-----------\\n\" \\\n",
        "          + \"Epoch: {}\".format(epoch + 1) + \"/{}\".format(n_epochs) \\\n",
        "          + \" | Iteration: {}\".format(step) \\\n",
        "          + \" | Loss {:.5f}\".format(running_loss) \\\n",
        "          + \"\\n-----------\\n\"\n",
        "        pred_string = predict_text(rnn, seq_len=predict_len)\n",
        "        print(out_string)\n",
        "        print(pred_string)\n",
        "        with open(outname, \"a\") as f:\n",
        "          f.write(\"\\n\" + str(datetime.now()))\n",
        "          f.write(out_string)\n",
        "          f.write(pred_string)\n",
        "        \n",
        "        rnn.train()\n",
        "  rnn.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PrR6TWa7EFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "8a42792a-0270-43d1-f0da-e4745c2fde17"
      },
      "source": [
        "charRNN = CharRNN(hidden_size=512)\n",
        "charRNN.to(dev)\n",
        "train(charRNN, predict_len=256)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-91a48624a071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcharRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcharRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-166-debeed2e0637>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, n_epochs, learning_rate, print_every, batch_size, seq_len, predict_len)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;31m# in order to not go through entire history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-163-4b4876102d90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, h)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_characters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# X is now of shape [seq_len * batch_size, num_chars]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMc9aekJNQPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}