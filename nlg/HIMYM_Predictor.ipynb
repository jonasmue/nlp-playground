{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HIMYM_Predictor.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonasmue/nlp-playground/blob/master/nlg/HIMYM_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YyKpfXsZ80j",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Initial Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD-N15EyZm9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j3V3jb-dT1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1yImB5Z15J",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhvPDsOFaCNz",
        "colab_type": "text"
      },
      "source": [
        "### Load Data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFnf9LDYacuW",
        "colab_type": "code",
        "outputId": "6cd5f05c-a28d-451c-feae-476b94c7d887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIgclcEpag24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = os.path.join(\"drive\", \"My Drive\", \"data\", \"himym.txt\")\n",
        "with open(data_path, \"r\") as input_file:\n",
        "  text = input_file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eId6V1_bH0q",
        "colab_type": "code",
        "outputId": "be9ecdf4-fa93-48a5-c619-563597afc299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print the first letters of the text\n",
        "text[:150]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n01x01 - Pilot\\n\\n\\nPilot\\nScene One\\n[Title: The Year 2030]\\nNarrator: Kids, I'm going to tell you an incredible story. The story of how I met your mothe\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phKlEJgFaGW0",
        "colab_type": "text"
      },
      "source": [
        "### Turn Data into Label Encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzCs9k1RbY3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convenience Dictionaries\n",
        "characters = set(text)\n",
        "id2char = dict(enumerate(characters))\n",
        "char2id = {c:i for i,c in enumerate(characters)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVSUu3DJbfCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert char2id[id2char[5]] == 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcWbdjCxcM0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_characters = len(characters)\n",
        "text_labels = [char2id[c] for c in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSEeBaUfcXRU",
        "colab_type": "code",
        "outputId": "b30e8840-e5f6-45b2-96be-69c679e6033a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"The text consists of {} distinct characters.\".format(num_characters))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text consists of 93 distinct characters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi6t2XiKaKFC",
        "colab_type": "text"
      },
      "source": [
        "### Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UIuqJVKcn8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(text_labels, num_characters):\n",
        "  eye = torch.eye(num_characters)\n",
        "  X = torch.zeros((text_labels.shape[0], text_labels.shape[1], num_characters))\n",
        "  for i, sentence_labels in enumerate(text_labels):\n",
        "    X[i] = eye[sentence_labels]\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFcuFKCQcowk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outputs tensor of X with shape [batch_size, seq_len, num_chars] and y with shape [batch_size, seq_len]\n",
        "def get_next_training_batch(labels=text_labels, num_chars=num_characters, seq_len=128, batch_size=32):\n",
        "  \"\"\"\n",
        "  Returns a training batch generator which itself returns batches with\n",
        "  tuples of the following format\n",
        "\n",
        "  X of shape [batch_size, seq_len, num_chars] (one-hot-encoded) and\n",
        "  y of shape [batch_size, seq_len] (label-encoded)\n",
        "\n",
        "  Arguments:\n",
        "    labels: label encodings of the text to create batches from\n",
        "    num_chars: the total number of characters\n",
        "    seq_len: the length of the character sequence of each batch\n",
        "    batch_size: the number of character sequences per batch\n",
        "  \"\"\"\n",
        "  for batch_offset in range(0, len(labels), batch_size * (seq_len + 1)):\n",
        "    if len(labels) < batch_offset + batch_size * (seq_len + 1):\n",
        "      return\n",
        "    batch = labels[batch_offset:batch_offset + batch_size * (seq_len + 1)]\n",
        "    X_text_labels = torch.Tensor([batch[i:i+seq_len] for i in range(0, len(batch), seq_len + 1)]).long()\n",
        "    X_one_hot = to_one_hot(X_text_labels, num_characters)\n",
        "    y_text_labels = torch.Tensor([batch[i+1:i+seq_len+1] for i in range(0, len(batch), seq_len + 1)]).long()\n",
        "    yield X_one_hot.to(dev), y_text_labels.to(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8859QXpTc-Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the implementation to see if it generates valid outpus\n",
        "X_sample, y_sample = next(get_next_training_batch(seq_len=8, batch_size=5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY8zp8rjdmpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert X_sample.shape == torch.Size([5, 8, num_characters])\n",
        "assert y_sample.shape == torch.Size([5, 8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfRWwpBXd1Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert X_sample[0, 1].argmax().item() == y_sample[0][0]\n",
        "assert X_sample[1, 2].argmax().item() == y_sample[1][1]\n",
        "assert X_sample[4, 7].argmax().item() == y_sample[4][6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or-yGW4se1gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_to_text(tensor):\n",
        "  \"\"\"\n",
        "  Converts a tensor representation back to a string representation.\n",
        "\n",
        "  Arguments:\n",
        "    tensor: a torch.Tensor object with the following shape:\n",
        "      3D: [batch_size, seq_len, num_chars]\n",
        "      2D: [batch_size, seq_len]\n",
        "      1D: [seq_len]\n",
        "  \"\"\"\n",
        "  if len(tensor.shape) == 3:\n",
        "    return tensor_to_text(tensor.argmax(dim=2))\n",
        "  if len(tensor.shape) == 2:\n",
        "    return [tensor_to_text(line) for line in tensor]\n",
        "  if len(tensor.shape) == 1:\n",
        "    return \"\".join([tensor_to_text(char_encoding) for char_encoding in tensor])\n",
        "  if len(tensor.shape) == 0:\n",
        "    return id2char[tensor.item()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exyHWyLQgv9E",
        "colab_type": "code",
        "outputId": "f9756f91-41cf-410a-f3c7-01ef145373c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"3D:\", tensor_to_text(X_sample))\n",
        "print(\"2D:\", tensor_to_text(y_sample))\n",
        "print(\"1D:\", tensor_to_text(torch.Tensor([char2id[\"J\"], char2id[\"o\"], char2id[\"n\"], char2id[\"a\"], char2id[\"s\"]])))\n",
        "print(\"0D:\", tensor_to_text(torch.tensor(char2id[\"J\"])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3D: ['\\n\\n\\n01x01', '- Pilot\\n', '\\nPilot\\nS', 'ene One\\n', 'Title: T']\n",
            "2D: ['\\n\\n01x01 ', ' Pilot\\n\\n', 'Pilot\\nSc', 'ne One\\n[', 'itle: Th']\n",
            "1D: Jonas\n",
            "0D: J\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0PT7MnzAGhi",
        "colab_type": "code",
        "outputId": "cb15532e-5325-47f9-dcae-f81ae9fe427c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.Tensor([15])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICUOVqmaaQgD",
        "colab_type": "text"
      },
      "source": [
        "# Custom GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs5Eq1h8kgam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUCell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Weights and Biases\n",
        "    # See https://en.wikipedia.org/wiki/Gated_recurrent_unit#Fully_gated_unit\n",
        "    \n",
        "    ## z\n",
        "    self.W_xz = nn.Parameter(torch.zeros(self.input_size, self.hidden_size))\n",
        "    self.U_hz = nn.Parameter(torch.zeros(self.hidden_size, self.hidden_size))\n",
        "    self.b_z = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "    ## r\n",
        "    self.W_xr = nn.Parameter(torch.zeros_like(self.W_xz))\n",
        "    self.U_hr = nn.Parameter(torch.zeros_like(self.U_hz))\n",
        "    self.b_r = nn.Parameter(torch.zeros_like(self.b_z))\n",
        "\n",
        "    ## h\n",
        "    self.W_xh = nn.Parameter(torch.zeros_like(self.W_xz))\n",
        "    self.U_hh = nn.Parameter(torch.zeros_like(self.U_hz))\n",
        "    self.b_h = nn.Parameter(torch.zeros_like(self.b_z))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for weight in self.parameters():\n",
        "      if len(weight.shape) > 1:\n",
        "        # Init matrices with random noise\n",
        "        nn.init.xavier_normal_(weight)\n",
        "      else:\n",
        "        # Init biases with zeros\n",
        "        nn.init.zeros_(weight)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    h = nn.Parameter(torch.zeros((batch_size, self.hidden_size))).to(dev)\n",
        "    nn.init.zeros_(h)\n",
        "    return h\n",
        "\n",
        "  def forward(self, x, h=None):\n",
        "    \"\"\"\n",
        "    Argument shapes:\n",
        "    x is of shape [batch_size, input_size]\n",
        "    h is of shape [batch_size, hidden_size]\n",
        "\n",
        "    Output shape:\n",
        "    h is of shape [batch_size, hidden_size]\n",
        "    \"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "\n",
        "    if h is None:\n",
        "      h = self.init_hidden(x.shape[0])\n",
        "    \n",
        "    z = torch.sigmoid(x.mm(self.W_xz) + h.mm(self.U_hz) + self.b_z)\n",
        "    r = torch.sigmoid(x.mm(self.W_xr) + h.mm(self.U_hr) + self.b_r)\n",
        "    h = z * h + (1 - z) * torch.tanh(x.mm(self.W_xh) + (r * h).mm(self.U_hh) + self.b_h)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HspvC9QdkB0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, num_characters=num_characters, hidden_size=512, batch_first=True, drop=0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_characters = num_characters\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_first = batch_first\n",
        "\n",
        "    self.cell = GRUCell(num_characters, hidden_size)\n",
        "    #self.cell = nn.GRU(num_characters, hidden_size)\n",
        "    self.dropout = nn.Dropout(drop)\n",
        "    self.dense = nn.Linear(self.hidden_size, self.num_characters)\n",
        "\n",
        "  def forward(self, X, h_0=None):\n",
        "    \"\"\"\n",
        "    Argument shapes:\n",
        "    X is of shape [batch_size, seq_len, num_chars] if self.batch_first\n",
        "    X is of shape [seq_len, batch_size, num_chars] if not self.batch_first \n",
        "    ---\n",
        "    h is of shape [batch_size, hidden_size]\n",
        "\n",
        "    Output shapes:\n",
        "    y_hat is of shape [batch_size * seq_len, num_chars]\n",
        "    h_t is of shape [batch_size, hidden_size]\n",
        "    \"\"\"\n",
        "    assert len(X.shape) == 3\n",
        "\n",
        "    # Put seq_len in the front\n",
        "    if self.batch_first:\n",
        "      X = X.permute(1, 0, 2)\n",
        "      # X is now of shape [seq_len, batch_size, num_chars]\n",
        "\n",
        "    h_t = h_0\n",
        "    output = torch.zeros((X.shape[0], X.shape[1], self.hidden_size)).to(dev)\n",
        "    for t, x_t in enumerate(X):\n",
        "      # Iterate over sequence\n",
        "      h_t = self.cell(x_t, h_t)\n",
        "      output[t] = h_t # [batch_size, hidden_size]\n",
        "    \n",
        "    # TODO: Permute output back?!\n",
        "    output = output.permute(1, 0, 2)\n",
        "\n",
        "    output = output.contiguous().view(-1, self.hidden_size) # [batch_size * seq_len, hidden_size]\n",
        "    output = self.dropout(output)\n",
        "    y_hat = self.dense(output) # [batch_size * seq_len, num_chars]\n",
        "    return y_hat, h_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HdjU5HUaUD2",
        "colab_type": "text"
      },
      "source": [
        "# Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMFdO08lzO4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(rnn, char, h):\n",
        "  x = to_one_hot(torch.LongTensor([[char2id[char]]]), num_characters).to(dev)\n",
        "  y_hat, h = rnn(x, h)\n",
        "  next_char = tensor_to_text(torch.softmax(y_hat, dim=-1).argmax())[0]\n",
        "  return next_char, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjB4Yn44xpYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_text(rnn, h=None, seq_len=150, starting_with=\"\\n\"):\n",
        "  result = [c for c in starting_with]\n",
        "  for char in starting_with:\n",
        "    _, h = predict_next_char(rnn, char, h)\n",
        "  current_char = result[-1]\n",
        "  for i in range(seq_len):\n",
        "    current_char, h = predict_next_char(rnn, current_char, h)\n",
        "    result.append(current_char)\n",
        "  return \"\".join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK_ZjPMqaWvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rnn, n_epochs=50, learning_rate=2e-3, print_every=100, batch_size=64, seq_len=128, predict_len=150):\n",
        "  outname = os.path.join(\"drive\", \"My Drive\", \"results\", str(datetime.now()) + \".txt\")\n",
        "  with open(outname, \"w\") as f:\n",
        "    f.write(\"Training {}, num layers: {}, hidden size: {}, batch size: {}, sequence length: {}\".format(str(datetime.now()), 1, rnn.hidden_size, batch_size, seq_len))\n",
        "\n",
        "  rnn.train()\n",
        "\n",
        "  step = 0\n",
        "  losses = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    h = None\n",
        "    for X, y in get_next_training_batch(seq_len=seq_len, batch_size=batch_size):\n",
        "      step += 1\n",
        "      rnn.zero_grad()\n",
        "      \n",
        "      y_hat, h = rnn(X, h)\n",
        "      h = h.data # in order to not go through entire history\n",
        "\n",
        "      loss = criterion(y_hat, y.view(batch_size * seq_len))\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradient clipping\n",
        "      nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
        "      optimizer.step()\n",
        "\n",
        "      if not step % print_every:\n",
        "        rnn.eval()\n",
        "        running_loss = sum(losses) / len(losses)\n",
        "        losses = []\n",
        "        out_string = \"\\n-----------\\n\" \\\n",
        "          + \"Epoch: {}\".format(epoch + 1) + \"/{}\".format(n_epochs) \\\n",
        "          + \" | Iteration: {}\".format(step) \\\n",
        "          + \" | Loss {:.5f}\".format(running_loss) \\\n",
        "          + \"\\n-----------\\n\"\n",
        "        pred_string = predict_text(rnn, seq_len=predict_len)\n",
        "        print(out_string)\n",
        "        print(pred_string)\n",
        "        with open(outname, \"a\") as f:\n",
        "          f.write(\"\\n\" + str(datetime.now()))\n",
        "          f.write(out_string)\n",
        "          f.write(pred_string)\n",
        "        \n",
        "        rnn.train()\n",
        "  rnn.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PrR6TWa7EFp",
        "colab_type": "code",
        "outputId": "5e2e094c-c790-4de6-b3c4-1cc26c324563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "charRNN = CharRNN(hidden_size=1024)\n",
        "charRNN.to(dev)\n",
        "train(charRNN, n_epochs=75, predict_len=256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "Epoch: 1/75 | Iteration: 100 | Loss 2.82430\n",
            "-----------\n",
            "\n",
            "\n",
            "Ted: Oh, bing the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
            "\n",
            "-----------\n",
            "Epoch: 1/75 | Iteration: 200 | Loss 2.13560\n",
            "-----------\n",
            "\n",
            "\n",
            "Ted: I was the bat in the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the back of the ba\n",
            "\n",
            "-----------\n",
            "Epoch: 1/75 | Iteration: 300 | Loss 2.07247\n",
            "-----------\n",
            "\n",
            "\n",
            "Marshall and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "-----------\n",
            "Epoch: 2/75 | Iteration: 400 | Loss 1.85119\n",
            "-----------\n",
            "\n",
            "\n",
            "Marshall: What was the story on the bar the story on the bar the story of the story on the bar the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the s\n",
            "\n",
            "-----------\n",
            "Epoch: 2/75 | Iteration: 500 | Loss 1.69648\n",
            "-----------\n",
            "\n",
            "\n",
            "Ted: I want to do the best with the bar.\n",
            "Ted: I want to do the best with the bar.\n",
            "Ted: I want to do the best with the bar.\n",
            "Ted: I want to do the best with the bar.\n",
            "Ted: I want to do the best with the bar.\n",
            "Ted: I want to do the best with the bar.\n",
            "Ted: I wan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czvxsLFV0hE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = os.path.join(\"drive\", \"My Drive\", \"results\", str(datetime.now))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxQnxmlwnI1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(charRNN.state_dict(), os.path.join(model_path, \"model-state.pth\"))\n",
        "pickle.dump(char2id, os.path.join(model_path, \"char2id\"))\n",
        "pickle.dump(id2char, os.path.join(model_path, \"id2char\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okxbnm_j0-vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charRNN = CharRNN(hidden_size=1024).cuda()\n",
        "charRNN.load_state_dict(torch.load(os.path.join(model_path, \"model-state2020-05-27 20:29:25.892763.pth\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYLNcGaFBb42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(predict_text(charRNN, seq_len=1500, starting_with=\"Ted: Kids\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgOt1Mp9JVCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}